{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording finished.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import wave\n",
    "import pyaudio\n",
    "\n",
    "# Set up audio recording parameters\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "\n",
    "# Create an instance of PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open a new stream for recording\n",
    "stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "# Create a buffer to store the recorded audio\n",
    "frames = []\n",
    "\n",
    "# Record the audio for the specified number of seconds\n",
    "print(\"Recording started...\")\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "print(\"Recording finished.\")\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Save the recorded audio as a WAV file\n",
    "with wave.open(\"recorded_audio.wav\", \"wb\") as wav_file:\n",
    "    wav_file.setnchannels(CHANNELS)\n",
    "    wav_file.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wav_file.setframerate(RATE)\n",
    "    wav_file.writeframes(b\"\".join(frames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe('recorded_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Would you mind opening Google?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import speech_recognition as sr\n",
    "# import os\n",
    "# from email.message import EmailMessage\n",
    "# import ssl\n",
    "# import smtplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = sr.Recognizer()\n",
    "\n",
    "# def recognize_command():\n",
    "   \n",
    "#     command = result['text']\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         print(\"You said: \" + command)\n",
    "#         if \"Open\" in command:\n",
    "#             app = command.split(' ')[-1]\n",
    "#             os.system(f'start {app}')\n",
    "        \n",
    "           \n",
    "#     except sr.UnknownValueError:\n",
    "#         print(\"Sorry, I didn't understand that.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said:  Open Chrome.\n"
     ]
    }
   ],
   "source": [
    "# recognize_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Fine-tune the BERT model on a text classification task\n",
    "train_data = [(\"open chrome\", 1), (\"launch firefox\", 1), (\"start notepad\", 1), (\"close the window\", 0)]\n",
    "train_texts = [text for text, label in train_data]\n",
    "train_labels = [label for text, label in train_data]\n",
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "train_labels = torch.tensor(train_labels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**train_encodings, labels=train_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Use the fine-tuned model to classify a user's command\n",
    "text = 'I am surya'\n",
    "encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "outputs = model(**encoding)\n",
    "predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "if predicted_label == 1:\n",
    "    # launch the Chrome browser\n",
    "    subprocess.call([\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"])\n",
    "\n",
    "else: \n",
    "    print('I am unable to do it! Sorry!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # Load the BERT tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Get user input\n",
    "# user_input = input(\"Enter a command: \")\n",
    "\n",
    "# # Tokenize the user input and generate BERT embeddings\n",
    "# tokens = tokenizer(user_input, return_tensors=\"pt\")\n",
    "# outputs = model(**tokens)[0]\n",
    "\n",
    "# # Extract the relevant keywords and phrases from the BERT embeddings\n",
    "# keywords = []\n",
    "# for i in range(len(outputs)):\n",
    "#     if outputs[0][i] > 0.5:\n",
    "#         keywords.append(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokens[0][i])))\n",
    "        \n",
    "# # Print the extracted keywords\n",
    "# print(\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statement is not a command to open Chrome.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set up input text and convert it to tokens\n",
    "input_text = \"Open Chrome\"\n",
    "input_tokens = tokenizer.encode_plus(input_text, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "# Feed the input tokens to the BERT model and get the predicted class\n",
    "outputs = model(input_tokens['input_ids'], token_type_ids=input_tokens['token_type_ids'], attention_mask=input_tokens['attention_mask'])\n",
    "predicted_class = outputs.logits.argmax().item()\n",
    "\n",
    "# Map the predicted class to 0 or 1\n",
    "if predicted_class == 0:\n",
    "    print(\"The statement is not a command to open Chrome.\")\n",
    "else:\n",
    "    print(\"The statement is a command to open Chrome.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 629/629 [00:00<00:00, 79.6kB/s]\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 268M/268M [01:05<00:00, 4.09MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 7.98kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 327kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make a call\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the input text\n",
    "input_text = \"Open Google Chrome\"\n",
    "encoded_input = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Predict the label for the input\n",
    "outputs = model(encoded_input)\n",
    "predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "# Interpret the output\n",
    "if predicted_label == 0:\n",
    "    print(\"Open Google Chrome\")\n",
    "elif predicted_label == 1:\n",
    "    print(\"Make a call\")\n",
    "else:\n",
    "    print(\"Unknown command\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import keyring\n",
    "\n",
    "# Set your Hugging Face API key\n",
    "api_key = \"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\"\n",
    "\n",
    "# Set the keyring service name and account name\n",
    "service_name = \"huggingface\"\n",
    "account_name = \"api_key\"\n",
    "\n",
    "# Store the API key securely in the keyring\n",
    "keyring.set_password(service_name, account_name, api_key)\n",
    "\n",
    "# Retrieve the API key from the keyring\n",
    "api_key = keyring.get_password(service_name, account_name)\n",
    "\n",
    "\n",
    "# api_secret = openai_secret_manager.get_secret(\"openai\") \n",
    "# api_token = api_secret[\"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\"]\n",
    "API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/msmarco-distilbert-base-tas-b\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "data = query(\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"source_sentence\": \"That is a happy person\",\n",
    "            \"sentences\": [\n",
    "                \"That is a happy dog\",\n",
    "                \"That is a very happy person\",\n",
    "                \"Today is a sunny day\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    # [0.853, 0.981, 0.655]\n",
    ")\n",
    "\n",
    "# import openai_secret_manager\n",
    "\n",
    "# # Load the API key from your OpenAI secrets\n",
    "# api_secret = openai_secret_manager.get_secret(\"openai\")  # Replace \"openai\" with the name of your OpenAI secrets API key\n",
    "# api_token = api_secret[\"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\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input('Enter your query : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example sentences to compare\n",
    "sentences = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "             \"A stitch in time saves nine.\",\n",
    "             \"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# Tokenize the sentences and get the embeddings\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Compute cosine similarities between pairs of sentences\n",
    "cos_similarities = torch.nn.functional.cosine_similarity(model_output[0], model_output[0])\n",
    "print(cos_similarities)\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code below for only picking commands like \"OPEN SOMETHING\" withing a whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model('base')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a static function records the speech and returns text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text():\n",
    "    import io\n",
    "    import wave\n",
    "    import pyaudio\n",
    "    import string\n",
    "    import whisper\n",
    "\n",
    "    # Set up audio recording parameters\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    CHUNK = 1024\n",
    "    RECORD_SECONDS = 5\n",
    "\n",
    "    # Create an instance of PyAudio\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open a new stream for recording\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    # Create a buffer to store the recorded audio\n",
    "    frames = []\n",
    "\n",
    "    # Record the audio for the specified number of seconds\n",
    "    print(\"Recording started...\")\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded audio as a WAV file\n",
    "    with wave.open(\"recorded_audio.wav\", \"wb\") as wav_file:\n",
    "        wav_file.setnchannels(CHANNELS)\n",
    "        wav_file.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wav_file.setframerate(RATE)\n",
    "        wav_file.writeframes(b\"\".join(frames))\n",
    "\n",
    "    ##Model code has been already run above\n",
    "\n",
    "    result = model.transcribe('recorded_audio.wav')\n",
    "\n",
    "    cleaned_result = result['text'].lower().strip().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "    return cleaned_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this function would record and return the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\whisper\\transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'could you please open google'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To be run whenever want to record\n",
    "ressssult = speech_to_text()\n",
    "ressssult"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will only hear the command \"Open sth\" within whole rubbish sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open google\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = ressssult\n",
    "# sentence = \"I wanted to gather some information so if you could open google, I would be really happy\"\n",
    "match = re.search(r'open\\s+\\w+', sentence)\n",
    "\n",
    "if match:\n",
    "    command = match.group()\n",
    "    print(command)\n",
    "else:\n",
    "    print(\"Command not found\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for listening continuously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said: X1 is necessary for\n",
      "Could not understand audio\n",
      "No audio input\n",
      "Could not understand audio\n",
      "No audio input\n",
      "Could not understand audio\n",
      "No audio input\n",
      "Could not understand audio\n",
      "No audio input\n",
      "Could not understand audio\n",
      "No audio input\n",
      "Could not understand audio\n",
      "No audio input\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     text \u001b[39m=\u001b[39m speech_to_text()\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m         command \u001b[39m=\u001b[39m parse_command(text)\n",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m, in \u001b[0;36mspeech_to_text\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspeech_to_text\u001b[39m():\n\u001b[0;32m      5\u001b[0m     r \u001b[39m=\u001b[39m sr\u001b[39m.\u001b[39mRecognizer()\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mwith\u001b[39;00m sr\u001b[39m.\u001b[39;49mMicrophone() \u001b[39mas\u001b[39;00m source:\n\u001b[0;32m      7\u001b[0m         audio \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mlisten(source, phrase_time_limit\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      8\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\speech_recognition\\__init__.py:81\u001b[0m, in \u001b[0;36mMicrophone.__init__\u001b[1;34m(self, device_index, sample_rate, chunk_size)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39m# set up PyAudio\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaudio_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_pyaudio()\n\u001b[1;32m---> 81\u001b[0m audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpyaudio_module\u001b[39m.\u001b[39;49mPyAudio()\n\u001b[0;32m     82\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     count \u001b[39m=\u001b[39m audio\u001b[39m.\u001b[39mget_device_count()  \u001b[39m# obtain device count\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyaudio.py:677\u001b[0m, in \u001b[0;36mPyAudio.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    675\u001b[0m     \u001b[39m\"\"\"Initialize PortAudio.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     pa\u001b[39m.\u001b[39;49minitialize()\n\u001b[0;32m    678\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streams \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "def speech_to_text():\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        audio = r.listen(source, phrase_time_limit=5)\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print(f\"You said: {text}\")\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results; {e}\")\n",
    "            return None\n",
    "\n",
    "def parse_command(text):\n",
    "    if \"open\" in text:\n",
    "        command = text.split(\"open\")[1].strip()\n",
    "        return command\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "while True:\n",
    "    text = speech_to_text()\n",
    "    if text is not None:\n",
    "        command = parse_command(text)\n",
    "        if command is not None:\n",
    "            os.system(f\"open {command}\")  # or any other command you want to execute\n",
    "    else:\n",
    "        print(\"No audio input\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58dbfdb34cf82127b32c5737e6183911655ff227e5c11e8f5e4b25048ae98ef2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
